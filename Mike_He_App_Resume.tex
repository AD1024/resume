\documentclass{resume}
\usepackage{hyperref}

\usepackage[left=0.40in,top=-0.2in,right=0.40in,bottom=0.3in]{geometry}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{natbib, bibentry}
\titlespacing*{\rSection}{0pt}{1.1\baselineskip}{\baselineskip}
% \name{Mike He}
% \address{1135 NE CAMPUS PKWY \\ Seattle, WA 98105}
% \address{(206)~$\cdot$~887~$\cdot$~8588 \\ dh63@cs.washington.edu}
% \address{https://ad1024.space}
\newcommand{\myul}[2][blue]{\setulcolor{#1}\ul{#2}\setulcolor{blue}}
\usepackage{xcolor,soul,lipsum}

\begin{document}
\bibliographystyle{unsrt}
\nobibliography{publication.bib}
	\MakeUppercase{\Large{\textbf{Mike He}}} \hfill {\em{\href{mailto:dh63@cs.washington.edu}{dh63@cs.washington.edu}}}\\
	\vspace{-5pt}\href{https://ad1024.space}{https://ad1024.space} \hfill{\em (206)~$\cdot$~887~$\cdot$~8588}


%----------------------------------------------------------------------------------------
%	Specifications
%----------------------------------------------------------------------------------------

	% \begin{rSection}{Skills}
	% 	\begin{tabular}{ @{} >{\bfseries}l @{\hspace{4ex}} l }
	% 		Languages & C/C++, Python, Java, Rust, OCaml, Haskell, Coq, Agda, \LaTeX \\
	% 		Skills    & Certified \& Functional Programming, Automated Verification \\
	% 		Others    & I've been playing the violin for 17 years. I like Symphonies composed by \href{https://imslp.org/wiki/Category:Mahler,_Gustav}{Gustav Mahler}.
	% 	\end{tabular}
	% \end{rSection}
	% \vspace{-5pt}
%----------------------------------------------------------------------------------------
%	EDUCATION
%----------------------------------------------------------------------------------------

	\begin{rSection}{Education}
	{\bf University of Washington, Seattle} \hfill {\em Sept. 2018---Est. Jun. 2022} \\
	\textit{B.S. in Computer Science}
	\vspace{-5pt}
        \begin{itemize}[leftmargin=*]
            \setlength{\itemsep}{1pt}
            \setlength{\parskip}{0pt}
			\setlength{\parsep}{0pt}
			\item Cumulative GPA: 3.88 / 4
            \item Field of Studies: Programming Languages \& Formal Verification \& Compilers \& MLSys
		\end{itemize}
	\end{rSection}
	\vspace{-5pt}
    \begin{rSection}{Publications}
		\begin{enumerate}
			\setlength{\itemsep}{1pt}
            \setlength{\parskip}{0pt}
			\setlength{\parsep}{0pt}
			\item \bibentry{kirisame2021dynamic} (\small{*: Equal Contribution})
			\item \bibentry{latte21} (\small{*: Equal Contribution})
		\end{enumerate}
        \vspace{-5pt}
	\end{rSection}
    \vspace{-5pt}
    \begin{rSection}{Service}
    	\begin{enumerate}
    		\setlength{\itemsep}{1pt}
            \setlength{\parskip}{0pt}
    		\setlength{\parsep}{0pt}
    		\item[$\rightarrow$] \textbf{MICRO '21}, Artifact Evaluation
    		\item[$\rightarrow$] \textbf{CSE 505 - Principles of Programming Languages (Spring 2021)}, Teaching Assistant
    	\end{enumerate}
	\end{rSection}
    \vspace{-5pt}
%----------------------------------------------------------------------------------------
%	WORK EXPERIENCE
%----------------------------------------------------------------------------------------

	% \begin{rSection}{Experience}
		% \begin{rSubsection}{PLSE Lab, University of Washington}{Oct. 2019---Now}{Research Assistant}{Seattle, WA} 
		% 	\item \textbf{3LA: Verifiable Compiler Support for Domain-specific Accelerators}:\\\href{https://capra.cs.cornell.edu/latte21/paper/30.pdf}{\color{blue} \myul{3LA}} proposes an end-to-end compilation flow that provides \textbf{flexible} and \textbf{verifiable} support for custom Deep Learning (\textbf{DL}) accelerators. 3LA has a builtin implementation agnostic pattern matching algorithm that is capable of find accelerator supported workloads in DL models leveraging the power of Equality Saturation. Moreover, 3LA addresses the mapping gap between DL models represented in high-level domain-specific languages (DSLs) and target DL accelerators by using Instruction-level Abstraction (\textbf{ILA}) as the software-hardware interface. Because ILA models formal semantics of the target accelerator and can be verified against the RTL implementation, the interface opens up spaces for verification on the correctness of the compiler mapping.\\\textbf{Talks \& Presentations}:
		% 	\vspace{-5pt}
        %     \begin{enumerate}
        %         \setlength{\itemsep}{1pt}
        %         \setlength{\parskip}{0pt}
        %         \setlength{\parsep}{0pt}
        %         \item \textit{From DSLs to Accelerator-rich Platform: Addressing the Mapping Gap}, Sept. 2021 at Intel (presented jointly with \href{https://homes.cs.washington.edu/~sslyu/}{\color{blue} \myul {Steven Lyubomirsky}})
        %         \item \textit{Correct \& Flexible Support for Custom Accelerators}, Sept. 2021 at SRC ADA Center
        %     \end{enumerate}
        %     \item \textbf{Dynamic Tensor Rematerialization}:\\\href{https://github.com/uwsampl/dtr-prototype}{\color{blue} \myul{Dynamic Tensor Rematerialization}} (\textbf{DTR}) is an greedy gradient checkpointing algorithm. DTR \textbf{enables} training Deep Learning (\textbf{DL}) models on memory-constrained devices (e.g. GPUs, FPGA-based accelerators). Unlike previous approaches, DTR does not need any information of the DL model architectures ahead-of-time; instead it saves memory by evicting and recomputing tensors \textbf{on-the-fly}, i.e. trading time for memory, which further exploit opportunities of using gradient checkpointing on DL trainings. DTR is comparably efficient as previous approaches: it requires only $\mathcal{O}(N)$ more forward computations when training a $N$-layer linear feed-forward neural network with an $\Omega(\sqrt{N})$ memory budget.
		% \end{rSubsection}
		% \vspace{-5pt}
    \begin{rSection}{Experience}
        \begin{rSubsection}{3LA, LATTE '21}{June. 2020---Now}{Research Assistant @ PLSE}{Seattle, WA}
            \item \href{https://capra.cs.cornell.edu/latte21/paper/30.pdf}{\color{blue} \myul{3LA}} proposes an end-to-end compilation flow that provides \textbf{flexible} and \textbf{verifiable} support for custom Deep Learning (\textbf{DL}) accelerators. 3LA has a builtin implementation agnostic pattern matching algorithm that is capable of find accelerator supported workloads in DL models leveraging the power of Equality Saturation. Moreover, 3LA addresses the mapping gap between DL models represented in high-level domain-specific languages (DSLs) and target DL accelerators by using Instruction-level Abstraction (\textbf{ILA}) as the software-hardware interface. Because ILA models the formal semantics of the target accelerator and can be verified against the RTL implementation, the interface opens up spaces for verification on the correctness of the compiler mapping.
            \item \textbf{Talks \& Presentations}:
            \vspace{-5pt}
                \begin{enumerate}
                    \setlength{\itemsep}{1pt}
                    \setlength{\parskip}{0pt}
                    \setlength{\parsep}{0pt}
                    \item \textit{From DSLs to Accelerator-rich Platform: Addressing the Mapping Gap}, Sept. 2021 at Intel (presented jointly with \href{https://homes.cs.washington.edu/~sslyu/}{\color{blue} \myul {Steven Lyubomirsky}})
                    \item \textit{Correct \& Flexible Support for Custom Accelerators}, Sept. 2021 at SRC ADA Center
                \end{enumerate} 
        \end{rSubsection}
        \vspace{-5pt}
        \begin{rSubsection}{Dynamic Tensor Rematerialization, ICLR '21}{Oct. 2019---Aug. 2021}{Research Assistant @ PLSE}{Seattle, WA}
            \item \href{https://github.com/uwsampl/dtr-prototype}{\color{blue} \myul{Dynamic Tensor Rematerialization}} (\textbf{DTR}) is an greedy gradient checkpointing algorithm. DTR \textbf{enables} training Deep Learning (\textbf{DL}) models on memory-constrained devices (e.g. GPUs, FPGA-based accelerators). Unlike previous approaches, DTR does not need any information of the DL model architectures ahead-of-time; instead it saves memory by evicting and recomputing tensors \textbf{on-the-fly}, i.e. trading time for memory, which further exploit opportunities of using gradient checkpointing on DL trainings. DTR is comparably efficient as previous approaches: it requires only $\mathcal{O}(N)$ more forward computations when training a $N$-layer linear feed-forward neural network with an $\Omega(\sqrt{N})$ memory budget.
        \end{rSubsection}
        \vspace{-5pt}
		\begin{rSubsection}{Paul G. Allen School, University of Washington}{Mar. 2021---June. 2021}{Teaching Assistant}{Seattle, WA}
			\item Worked as TA for \textbf{Principles of Programming Languages} (CSE 505)
			\item Helped re-designing CSE 505 and developing course materials for various topics about PL and formal verification (\textbf{Hoare Logic}, \textbf{Lambda Calculus} and \textbf{System F}, etc.) in \textbf{Coq}.
			\item Held office hours on weekends and shared tricks used in Coq tactics and Coq programming.
			\item Coordinate grading of all homework assignments.
		\end{rSubsection}
    \end{rSection}
	\vspace{-5pt}
	% \newpage
	%----------------------------------------------------------------------------------------
	%	PROJECTS
	%----------------------------------------------------------------------------------------

	% \begin{rSection}{Projects}
	% 	%------------------------------
	% 	%      veripy
	% 	%------------------------------
	% 	\textbf{\href{https://github.com/AD1024/veripy}{\color{blue} \myul{veripy}}} % \hfill {\em {\href{https://github.com/AD1024/veripy}{On GitHub}}}
	% 	\vspace{-5pt}

	% 	\begin{itemize}
	% 		\setlength{\itemsep}{1pt}
    %         \setlength{\parskip}{0pt}
	% 		\setlength{\parsep}{0pt}
	% 		\item An easy-to-use auto-active program verification library for Python programs written in \textbf{Python}.
	% 		\item The library is shallowly embedded in Python and the interface is implemented as \textbf{decorators}. It compiles annotated Python functions to \textbf{SMT} formulae and calls \textbf{SMT solver} to check whether it matches the given specification and gives a counter-example input when it violates any constraint.
	% 		% \item \textbf{Language} \& \textsc{Tools}: \textbf{Python 3}, \textbf{SMT-LIB}, \textsc{Z3}, \textsc{pyparsing}
	% 		% \item Keywords: SMT Solver, Static Analysis, Hoare Logic, Program Verification
	% 	\end{itemize}

	% 	%------------------------------
	% 	%      dtlc
	% 	%------------------------------
	% 	\textbf{\href{https://github.com/AD1024/dtlc}{\color{blue} \myul{dtlc}}} % \hfill {\em {\href{https://github.com/AD1024/dtlc}{On GitHub}}}
	% 	\vspace{-5pt}

	% 	\begin{itemize}
	% 		\setlength{\itemsep}{1pt}
    %         \setlength{\parskip}{0pt}
	% 		\setlength{\parsep}{0pt}
	% 		\item Implemented \textbf{dependently-typed} lambda calculus in Martin-Löf style intuitionistic type theory.
	% 		\item Written in \textbf{OCaml}, \textbf{dtlc} has a language frontend \textbf{Lexer \& Parser} implemented using \textbf{Menhir}. Core language supports \textbf{type unification} with \textbf{metavariables} which makes the type inference stronger.
	% 		\item Implemented eliminators for \textbf{naturals}, \textbf{identity type}, \textbf{union type}, etc.
	% 	\end{itemize}
	% \end{rSection}
\end{document}
